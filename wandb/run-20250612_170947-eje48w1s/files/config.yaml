_wandb:
    value:
        cli_version: 0.19.11
        m: []
        python_version: 3.11.9
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
            "3":
                - 16
                - 23
                - 55
            "4": 3.11.9
            "5": 0.19.11
            "6": 4.52.4
            "8":
                - 3
                - 5
            "12": 0.19.11
            "13": windows-amd64
data:
    value:
        max_length: 512
        path: data/processed/train.json
        split: train
        text_column: text
lora:
    value:
        alpha: 32
        dropout: 0.1
        r: 16
        target_modules:
            - q_proj
            - v_proj
model:
    value:
        max_length: 512
        name: microsoft/DialoGPT-medium
training:
    value:
        batch_size: 4
        epochs: 3
        eval_steps: 500
        evaluation_strategy: steps
        gradient_accumulation_steps: 4
        learning_rate: "1e-4"
        logging_steps: 10
        output_dir: ./results
        save_steps: 1000
        use_fp16: true
        warmup_steps: 100
wandb:
    value:
        entity: null
        project: llm-finetuning
