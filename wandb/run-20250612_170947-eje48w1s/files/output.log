tokenizer_config.json: 100%|████████████████████████████████████████████| 614/614 [00:00<?, ?B/s]
C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\jaswi\.cache\huggingface\hub\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
vocab.json: 100%|████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 631kB/s]
merges.txt: 100%|█████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.33MB/s]
config.json: 100%|███████████████████████████████████████████████| 642/642 [00:00<00:00, 932kB/s]
pytorch_model.bin: 100%|██████████████████████████████████████| 863M/863M [03:45<00:00, 3.83MB/s]
generation_config.json: 100%|███████████████████████████████████████████| 124/124 [00:00<?, ?B/s]
Traceback (most recent call last):
  File "c:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\src\models\train.py", line 117, in <module>
    main()
  File "c:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\src\models\train.py", line 80, in main
    model, tokenizer = prepare_model_and_tokenizer(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\src\models\train.py", line 43, in prepare_model_and_tokenizer
    model = get_peft_model(model, lora_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\mapping_func.py", line 123, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\peft_model.py", line 1722, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\peft_model.py", line 132, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\tuners\lora\model.py", line 142, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\tuners\tuners_utils.py", line 180, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "C:\Users\jaswi\Desktop\FILES\Projects\Code\LLm Fine-tuning\llm_finetune\Lib\site-packages\peft\tuners\tuners_utils.py", line 527, in inject_adapter
    raise ValueError(error_msg)
ValueError: Target modules {'v_proj', 'q_proj'} not found in the base model. Please check the target modules and try again.
model.safetensors: 100%|██████████████████████████████████████| 863M/863M [03:41<00:00, 3.90MB/s]
